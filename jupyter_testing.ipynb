{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAN\n",
    "1. Load up to Jun 2022 into BigQuery\n",
    "2. Create github action to run BQ insert script nightly\n",
    "3. Set grouped database table (history_df) in BigQuery to refresh nightly after insert\n",
    "4. Make script to pull data history_df from BigQuery data\n",
    "\n",
    "8. For streamlit app append [history_df, current_df, future_df] to make the combo_df\n",
    "    - put history_df into cache using st.experimental_memo so its only called once each time app is opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize python libs & SQL creads\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "import streamlit as st\n",
    "import pull_nrg_data\n",
    "import ab_power_trader\n",
    "import json\n",
    "import http.client\n",
    "import certifi\n",
    "import ssl\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull current day of data (current_df)\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pull_nrg_data\n",
    "from google.oauth2 import service_account\n",
    "from pandasql import sqldf\n",
    "\n",
    "def get_streams():\n",
    "    streams = pd.read_csv('stream_codes.csv')\n",
    "    lst = [int(id) for id in streams[(streams['timeInterval']=='1 hr') & (streams['intervalType']=='supply')]['streamId']]\n",
    "    return lst\n",
    "[86, 322684, 322677, 87, 85, 23695, 322665, 23694]\n",
    "# Path to Google auth credentials\n",
    "#credentials = service_account.Credentials.from_service_account_info(st.secrets[\"gcp_service_account\"])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    streamIds = [86, 322684, 322677, 87, 85, 23695, 322665, 23694]\n",
    "    current_df = pd.DataFrame([])\n",
    "    today = datetime.now()\n",
    "    for id in streamIds:\n",
    "        accessToken, tokenExpiry = pull_nrg_data.getToken()\n",
    "        try:\n",
    "            APIdata = pull_nrg_data.pull_data(today.strftime('%m/%d/%Y'), today.strftime('%m/%d/%Y'), id, accessToken, tokenExpiry)\n",
    "            pull_nrg_data.release_token(accessToken)\n",
    "            APIdata['timeStamp'] = pd.to_datetime(APIdata['timeStamp'])\n",
    "            current_df = pd.concat([current_df, APIdata], axis=0)\n",
    "        except:\n",
    "            pull_nrg_data.release_token(accessToken)\n",
    "            pass\n",
    "    query = '''\n",
    "        SELECT  \n",
    "                fuelType,\n",
    "                strftime('%Y', timeStamp) AS year,\n",
    "                strftime('%m', timeStamp) AS month,\n",
    "                strftime('%d', timeStamp) AS day,\n",
    "                strftime('%H', timeStamp) AS hour,\n",
    "                avg(value) \n",
    "        FROM current_df\n",
    "        GROUP BY fuelType, year, month, day, hour\n",
    "        '''\n",
    "sqldf(query, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_nrg_data.release_token(accessToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release token\n",
    "import certifi\n",
    "import ssl\n",
    "import http.client\n",
    "server = 'api.nrgstream.com'\n",
    "\n",
    "def release_token(accessToken):\n",
    "    path = '/api/ReleaseToken'\n",
    "    headers = {'Authorization': f'Bearer {accessToken}'}\n",
    "    context = ssl.create_default_context(cafile=certifi.where())\n",
    "    conn = http.client.HTTPSConnection(server,context=context)\n",
    "    conn.request('DELETE', path, None, headers)\n",
    "    res = conn.getresponse()\n",
    "    print('token released')\n",
    "\n",
    "accessToken = '6Bkess-sojuUO1hQp3v8yw95UtGAWTf1glDZ9HiIhGOdAOdwFxqJDapWeeq1roBpYE_6jMgbYENN5qe4nyaTjX5YsTETWBpPwRrNUl24ClVdJ3WYiDVnRfF_JuDkASPHluCNv3qvSuj4BNbZvsEzToR6AGxbqGPgySAy2uDPNb19HIMgGy7dVo53hH_5An33C3ockq5plH_k9Zs2FnED94EDr9DJ4OVFe5qT60DgIMkK3DsHrnEZmvmvhH2Igu3rpMp8myfgLi7Z3xeAVWEv1VlNAX9x9LuK7kHOdjekf_ZBWkW0clkFP4WnyjO420GzEmbDeY6ByerjsacekazqaoQgjzA'\n",
    "release_token(accessToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data from BQ\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "# Path to Google auth credentials\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/ryan-bulger/power-trader/google-big-query.json'\n",
    "sql = \"SELECT * FROM nrgdata.grouped_data\"\n",
    "df = bigquery.Client().query(sql).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_data(streamIds, streamNames, years):\n",
    "    stream_df = pd.DataFrame([])\n",
    "    for id in streamIds:\n",
    "        server = 'api.nrgstream.com'\n",
    "        year_df = pd.DataFrame([])\n",
    "        for yr in years:\n",
    "            accessToken, tokenExpiry = pull_nrg_data.getToken()\n",
    "            # Define start & end dates\n",
    "            startDate = date(yr,1,1).strftime('%m/%d/%Y')\n",
    "            endDate = date(yr,12,31).strftime('%m/%d/%Y')\n",
    "            # NRG API connection\n",
    "            path = f'/api/StreamData/{id}?fromDate={startDate}&toDate={endDate}'\n",
    "            headers = {'Accept': 'Application/json', 'Authorization': f'Bearer {accessToken}'}\n",
    "            context = ssl.create_default_context(cafile=certifi.where())\n",
    "            conn = http.client.HTTPSConnection(server, context=context)\n",
    "            conn.request('GET', path, None, headers)\n",
    "            res = conn.getresponse()\n",
    "            # Load json data from API & create pandas df\n",
    "            jsonData = json.loads(res.read().decode('utf-8'))\n",
    "            df = pd.json_normalize(jsonData, record_path='data')\n",
    "            # Close NRG API connection\n",
    "            conn.close()\n",
    "            # Concat years for each stream\n",
    "            year_df = pd.concat([year_df,df], axis=0)\n",
    "            # Release NRG API access token\n",
    "            pull_nrg_data.release_token(accessToken)\n",
    "        # Rename year_df cols\n",
    "        year_df.rename(columns={0:'timeStamp', 1:f'{streamNames[id]}'}, inplace=True)\n",
    "        print(year_df)\n",
    "        # Change timeStamp to datetime\n",
    "        year_df['timeStamp'] = pd.to_datetime(year_df['timeStamp'])\n",
    "        # Re-index the year_df\n",
    "        year_df.set_index('timeStamp', inplace=True)\n",
    "        # Join year_df to outages dataframe\n",
    "        stream_df = pd.concat([stream_df,year_df], axis=1, join='outer')\n",
    "    return stream_df\n",
    "\n",
    "streamIds = [44648, 118361, 322689, 118362, 147262, 322675, 322682, 44651]\n",
    "#streamIds = [44648]\n",
    "streamNames = {44648:'Coal', 118361:'Gas', 322689:'Dual Fuel', 118362:'Hydro', 147262:'Wind', 322675:'Solar', 322682:'Energy Storage', 44651:'Biomass & Other'}\n",
    "years = [datetime.now().year, datetime.now().year+1, datetime.now().year+2]\n",
    "outage_df = stream_data(streamIds, streamNames, years)\n",
    "#print(outage_df)\n",
    "# Reset index so dataframe can be plotted with Altair\n",
    "outage_df.reset_index(inplace=True)\n",
    "outage_df = pd.melt(outage_df, \n",
    "                id_vars=['timeStamp'],\n",
    "                value_vars=['Coal', 'Gas', 'Dual Fuel', 'Hydro', 'Wind', 'Solar', 'Energy Storage', 'Biomass & Other'],\n",
    "                var_name='Source',\n",
    "                value_name='Value')\n",
    "outage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining daily to hourly data\n",
    "import pandas as pd\n",
    "\n",
    "intraday = pd.DataFrame(index=pd.date_range('2016-01-01', '2016-01-07', freq='H'),data=[i for i in range(145)], columns=['hourly'])\n",
    "daily = pd.DataFrame(index=pd.date_range('2016-01-01', '2016-01-07', freq='D'), data=[i for i in range(7)], columns=['daily'])\n",
    "df = intraday.join(daily).fillna(method='ffill')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ryan.Bulger': '4035129991@msg.telus.com'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn = st.secrets['phone_numbers']\n",
    "pn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b2c7660418be0c1c726d92d797a7ce8f65e49b2683610c7a567c05150d58fed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
